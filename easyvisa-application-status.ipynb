{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"EasyVisa Project\n\n# Context:\n\nBusiness communities in the United States are facing high demand for human resources, but one of the constant challenges is identifying and attracting the right talent, which is perhaps the most important element in remaining competitive. Companies in the United States look for hard-working, talented, and qualified individuals both locally as well as abroad.\n\nThe Immigration and Nationality Act (INA) of the US permits foreign workers to come to the United States to work on either a temporary or permanent basis. The act also protects US workers against adverse impacts on their wages or working conditions by ensuring US employers' compliance with statutory requirements when they hire foreign workers to fill workforce shortages. The immigration programs are administered by the Office of Foreign Labor Certification (OFLC).\n\nOFLC processes job certification applications for employers seeking to bring foreign workers into the United States and grants certifications in those cases where employers can demonstrate that there are not sufficient US workers available to perform the work at wages that meet or exceed the wage paid for the occupation in the area of intended employment.\n\n\nIn FY 2016, the OFLC processed 775,979 employer applications for 1,699,957 positions for temporary and permanent labor certifications. This was a nine percent increase in the overall number of processed applications from the previous year. The process of reviewing every case is becoming a tedious task as the number of applicants is increasing every year.\n\nThe increasing number of applicants every year calls for a Machine Learning based solution that can help in shortlisting the candidates having higher chances of VISA approval. OFLC has hired your firm EasyVisa for data-driven solutions. You as a data scientist have to analyze the data provided and, with the help of a classification model:\n\n* Facilitate the process of visa approvals.\n* Recommend a suitable profile for the applicants for whom the visa should be certified or denied based on the drivers that significantly influence the case status. \n\n\n# Data Description\n\nThe data contains the different attributes of the employee and the employer. The detailed data dictionary is given below.\n\n* case_id: ID of each visa application\n* continent: Information of continent the employee\n* education_of_employee: Information of education of the employee\n* has_job_experience: Does the employee has any job experience? Y= Yes; N = No\n* requires_job_training: Does the employee require any job training? Y = Yes; N = No \n* no_of_employees: Number of employees in the employer's company\n* yr_of_estab: Year in which the employer's company was established\n* region_of_employment: Information of foreign worker's intended region of employment in the US.\n* prevailing_wage:  Average wage paid to similarly employed workers in a specific occupation in the area of intended employment. The purpose of the prevailing wage is to ensure that the foreign worker is not underpaid compared to other workers offering the same or similar service in the same area of employment. \n* unit_of_wage: Unit of prevailing wage. Values include Hourly, Weekly, Monthly, and Yearly.\n* full_time_position: Is the position of work full-time? Y = Full Time Position; N = Part Time Position\n* case_status:  Flag indicating if the Visa was certified or denied","metadata":{"id":"AT5OogJVFbwu"}},{"cell_type":"markdown","source":"## Importing Necessary Libraries","metadata":{"id":"dirty-island"}},{"cell_type":"code","source":"# This command will make Python code more structured\n%load_ext nb_black\n\n# Make warnings not displayed\nimport warnings\n\nwarnings.filterwarnings(\"ignore\")\nfrom statsmodels.tools.sm_exceptions import ConvergenceWarning\n\nwarnings.simplefilter(\"ignore\", ConvergenceWarning)\n\n# Libraries for reading and manipulating data\nimport pandas as pd\nimport numpy as np\n\n# Library for splitting data\nfrom sklearn.model_selection import train_test_split\n\n# Libaries for data visualization\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\n# Set limits on number of displayed columns and rows\npd.set_option(\"display.max_columns\", None)  # no maximum limit\npd.set_option(\"display.max_rows\", 200)  # maximum of 200 rows\n\n# Library for building and showing decision tree models\nfrom sklearn.tree import DecisionTreeClassifier\nfrom sklearn.tree import plot_tree\n\n# Library for Bagging ensemble technique\nfrom sklearn.ensemble import BaggingClassifier\n\n# Library for Random Forest ensemble technique\nfrom sklearn.ensemble import RandomForestClassifier\n\n# Library for AdaBoost ensemble technique\nfrom sklearn.ensemble import AdaBoostClassifier\n\n# Library for Gradient Boosting ensemble technique\nfrom sklearn.ensemble import GradientBoostingClassifier\n\n# Library for XGBoost ensemble technique\nfrom xgboost import XGBClassifier\n\n# Library for Stacking ensemble technique\nfrom sklearn.ensemble import StackingClassifier\n\n# To tune different models\nfrom sklearn.model_selection import GridSearchCV\n\n# Libraries for calculating different metric scores\nfrom sklearn.metrics import (\n    f1_score,\n    accuracy_score,\n    recall_score,\n    precision_score,\n    make_scorer,\n    confusion_matrix,\n)","metadata":{"id":"statewide-still"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Importing and Checking Data","metadata":{"id":"desperate-infection"}},{"cell_type":"code","source":"# Read data and create a data frame\ndf_orig = pd.read_csv(\"EasyVisa.csv\")  # original data frame\n\n# Create a copy of original data frame for further steps\ndf_0 = df_orig.copy()","metadata":{"id":"persistent-juice"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Print size of data frame\nprint(\n    f\"There are {df_0.shape[0]} rows and {df_0.shape[1]} columns in the original data frame.\"\n)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Show sample rows of original data\ndf_0.sample(10, random_state=1)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"##### Observations\n- The column names all seem fine and do not need modification.\n- The column `case_id` could be removed, as it does not contain any data usable in the prediction models.\n- The values in the columns `has_job_experience`, `requires_job_training`, and `full_time_poistion` are *Y* or *N*, so they could be encoded as 1 and 0, respectively.\n- The education levels stored in the column `education_of_employee` could be replaced with ordinal integer values.\n- The variable `yr_of_estab` is hard to interpret, so it could be transformed into *years since establishment*.\n- The unit of `prevaliling_wage` is not constant, so it would make this parameter more interpretable if its unit is made constant. This will reduce the number of independent variables as `unit_of_wage` will be removed.","metadata":{}},{"cell_type":"code","source":"# Check for duplicate rows\ndplct_no = df_0.duplicated().sum()\nprint(f\"There are {dplct_no} duplicate rows in the data.\")","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Check types of data columns and number of non-null values in each column\ndf_0.info()","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"##### Observations\n- Considering that the total of rows is 25480, none of the columns have null/missing values.\n- Among the 11 columns of data (excluding `case_id`), 3 are of numeric type and the remaining 8 are of non-numeric type.\n    - Numeric:\n        - *Integer:* `no_of_employees` and `yr_of_estab`\n        - *Float:* `prevailing_wage`\n    - Non-numeric:\n        - *Object:* `continent`, `education_of_employee`, `has_job_experience`, `requires_job_training`, `region_of_employment`, `unit_of_wage`, `full_time_position`, and `case_status`","metadata":{}},{"cell_type":"code","source":"# Check statistical summary of numeric data\ndf_0.describe().T","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"##### Observations\n- The mean and median values of `no_of_employees` are 5667 and 2109, respectively, implying a right-skewed distribution.\n- The maximum value of `no_of_employees` is above 600000, which is quite high but possible.\n- The minimum value of `no_of_employees` is -26, i.e., negative, which is unreasonable. The negative values should be treated as missing values.\n- The oldest and newest employers have been established since (`yr_of_estab` =) 1800 and 2016, respectively.\n- The distribution of `prevailing_wage` is difficult to interpret at this point, because its unit varies across the rows. However, the minimum value is above zero, which is reasonable.","metadata":{}},{"cell_type":"code","source":"# Check statistical summary of non-numeric data\ndf_0.describe(include=[\"object\"]).T","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Identify unique values of categorical data columns\ncat_cols = df_0.select_dtypes(include=\"object\").columns  # columns of object data type\n\nfor col in cat_cols:\n    print(\"Unique values in the column\", col, \"are:\")\n    print(df_0[col].value_counts())\n    print(\"=\" * 60)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"##### Observations\n- The majority of employees are from *Asia*.\n- The majority of employees have a *Bachelor's* degree.\n- Most of the employees have job experience.\n- The vast majority of the jobs do not require training.\n- The regions *Northeast*, *South*, and *West* need most of the employees.\n- The available units for wage are *Year*, *Hour*, *Week*, and *Month*. The majority of the wage values in the data are per year.\n- The vast majority of the applications are for full-time positions.\n- Near 2/3 of the visa applications are certified.","metadata":{}},{"cell_type":"code","source":"# Drop case_id column before EDA, as it has no meaning for analyses and modeling\ndf_0.drop(\"case_id\", axis=1, inplace=True)","metadata":{"scrolled":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Exploratory Data Analysis (EDA)","metadata":{"id":"seasonal-calibration"}},{"cell_type":"markdown","source":"# a) Univariate Analysis","metadata":{}},{"cell_type":"markdown","source":"#### User-Defined Functions for Univariate Plots","metadata":{"id":"right-permit"}},{"cell_type":"code","source":"# User-defined function to plot a boxplot and a histogram along the same scale\ndef histogram_boxplot(\n    data, feature, xlabel, ylabel, figsize=(8, 6), kde=False, bins=None\n):\n    \"\"\"\n    Boxplot and histogram combined\n\n    data: dataframe\n    feature: dataframe column\n    xlabel: label of x-axis\n    ylabel: label of y-axis\n    figsize: size of figure (default (8, 6))\n    kde: whether to show the density curve (default False)\n    bins: number of bins for histogram (default None)\n    \"\"\"\n    f2, (ax_box2, ax_hist2) = plt.subplots(\n        nrows=2,  # Number of rows of the subplot grid= 2\n        sharex=True,  # x-axis will be shared among all subplots\n        gridspec_kw={\"height_ratios\": (0.25, 0.75)},\n        figsize=figsize,\n    )  # creating the 2 subplots\n\n    sns.boxplot(\n        data=data, x=feature, ax=ax_box2, showmeans=True, color=\"orange\"\n    )  # boxplot will be created and a star will indicate the mean value of the column\n\n    sns.histplot(\n        data=data, x=feature, kde=kde, ax=ax_hist2, bins=bins, palette=\"Set2\"\n    ) if bins else sns.histplot(\n        data=data, x=feature, kde=kde, ax=ax_hist2\n    )  # For histogram\n\n    ax_hist2.axvline(\n        data[feature].mean(), color=\"green\", linestyle=\"--\"\n    )  # Add mean to the histogram\n\n    ax_hist2.axvline(\n        data[feature].median(), color=\"red\", linestyle=\"-\"\n    )  # Add median to the histogram\n\n    ax_box2.set_xlabel(\"\", fontsize=16)  # remove label of 1st x-axis\n    ax_hist2.set_xlabel(xlabel, fontsize=16)  # set 2nd x-axis label\n    ax_hist2.set_ylabel(ylabel, fontsize=16)\n    # set y-axis label","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# User-defined function to create labeled barplots\ndef labeled_barplot(data, feature, xlabel, ylabel, perc=False, n=None):\n    \"\"\"\n    Barplot with percentage to the left\n\n    data: dataframe\n    feature: dataframe column\n    xlabel: label of x-axis\n    ylabel: label of y-axis\n    perc: whether to display percentages instead of count (default is False)\n    n: displays the top n category levels (default is None, i.e., display all levels)\n    \"\"\"\n\n    total = len(data[feature])  # length of the column\n    count = data[feature].nunique()\n    if n is None:\n        plt.figure(figsize=(8, 0.5 * count + 1))\n    else:\n        plt.figure(figsize=(8, 0.5 * n + 1))\n\n    plt.yticks(fontsize=14)\n    plt.xticks(fontsize=14)\n\n    ax = sns.countplot(\n        data=data,\n        y=feature,\n        palette=\"Set2\",\n        order=data[feature].value_counts().index[:n].sort_values(),\n    )\n\n    for p in ax.patches:\n        if perc == True:\n            label = \"{:.1f}%\".format(\n                100 * p.get_width() / total\n            )  # percentage of each class of the category\n        else:\n            label = p.get_width()  # count of each level of the category\n\n        y = p.get_y() + p.get_height() / 2\n        x = p.get_width()\n\n        ax.annotate(\n            label,\n            (x, y),\n            ha=\"left\",\n            va=\"center\",\n            size=12,\n            xytext=(0, 0),\n            textcoords=\"offset points\",\n        )  # annotate the percentage\n\n    ax.set_xlabel(xlabel, fontsize=16)  # set x-axis label\n    ax.set_ylabel(ylabel, fontsize=16)  # set y-axis label\n\n    plt.show()  # show the plot","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"#### Continent of Origin","metadata":{}},{"cell_type":"code","source":"# Use user-defined function labeled_barplot() to examine distribution of data\nlabeled_barplot(\n    data=df_0,\n    feature=\"continent\",\n    xlabel=\"Number of Applications\",\n    ylabel=\"Continent of Origin\",\n    perc=True,\n)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"##### Observations\n- The majority (66%) of the visa applicants are from *Asia*, which makes sense given the high population of this continent.\n- The lowest fraction (<1%) of the applicants are from *Oceania*, which also makes sense given its very low population.\n- *North America* and *Europe* have close number of applicants (12.9% and 14.6%).","metadata":{}},{"cell_type":"markdown","source":"#### Education Level","metadata":{}},{"cell_type":"code","source":"# Use user-defined function labeled_barplot() to examine distribution of data\nlabeled_barplot(\n    data=df_0,\n    feature=\"education_of_employee\",\n    xlabel=\"Number of Applications\",\n    ylabel=\"Education Level\",\n    perc=True,\n)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"##### Observations\n- The majority of the applicants have either bachelor's degrees (40.2%) or master's degrees (37.8%).\n- Only 8.6% of the applicants have doctorate degrees.","metadata":{}},{"cell_type":"markdown","source":"#### Job Experience","metadata":{}},{"cell_type":"code","source":"# Use user-defined function labeled_barplot() to examine distribution of data\nlabeled_barplot(\n    data=df_0,\n    feature=\"has_job_experience\",\n    xlabel=\"Number of Applications\",\n    ylabel=\"Job Experience\",\n    perc=True,\n)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"##### Observations\n- More than half (58%) of the applicants have job experience.","metadata":{}},{"cell_type":"markdown","source":"#### Job Training Requirement","metadata":{}},{"cell_type":"code","source":"# Use user-defined function labeled_barplot() to examine distribution of data\nlabeled_barplot(\n    data=df_0,\n    feature=\"requires_job_training\",\n    xlabel=\"Number of Applications\",\n    ylabel=\"Training Requirement\",\n    perc=True,\n)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"##### Observations\n- The vast majority (>88%) of the jobs do not require the applicants to receive training.","metadata":{}},{"cell_type":"markdown","source":"#### Employer Region","metadata":{}},{"cell_type":"code","source":"# Use user-defined function labeled_barplot() to examine distribution of data\nlabeled_barplot(\n    data=df_0,\n    feature=\"region_of_employment\",\n    xlabel=\"Number of Applications\",\n    ylabel=\"Employer Region\",\n    perc=True,\n)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"##### Observations\n- Most of the applications are for employment in the *Northeast*, *South*, and *West* regions of the United States. This could be expected because the majority of the tech companies are in those regions and the populations of those regions are higher than the other regions of the United States.\n- The *Island* region has the lowest number (1.5%) of work visa applicants.","metadata":{}},{"cell_type":"markdown","source":"#### Position Type","metadata":{}},{"cell_type":"code","source":"# Use user-defined function labeled_barplot() to examine distribution of data\nlabeled_barplot(\n    data=df_0,\n    feature=\"full_time_position\",\n    xlabel=\"Number of Applications\",\n    ylabel=\"Full-Time Position\",\n    perc=True,\n)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"##### Observations\n- More than 89% of the applications are related to full-time employment.","metadata":{}},{"cell_type":"markdown","source":"#### Wage Unit","metadata":{}},{"cell_type":"code","source":"# Use user-defined function labeled_barplot() to examine distribution of data\nlabeled_barplot(\n    data=df_0,\n    feature=\"unit_of_wage\",\n    xlabel=\"Number of Applications\",\n    ylabel=\"Wage Unit\",\n    perc=True,\n)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"##### Observations\n- The dominant majority (90%) of the applications are for the jobs whose prevailing wages are computed per year.","metadata":{}},{"cell_type":"markdown","source":"#### Case Status","metadata":{}},{"cell_type":"code","source":"# Use user-defined function labeled_barplot() to examine distribution of data\nlabeled_barplot(\n    data=df_0,\n    feature=\"case_status\",\n    xlabel=\"Number of Applications\",\n    ylabel=\"Case Status\",\n    perc=True,\n)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"##### Observations\n- Almost two-thirds of the visa applications are certified.","metadata":{}},{"cell_type":"markdown","source":"#### Number of Employees","metadata":{}},{"cell_type":"code","source":"# Use user-defined function histogram_boxplot() to examine distribution of data\nhistogram_boxplot(\n    data=df_0,\n    feature=\"no_of_employees\",\n    xlabel=\"Number of Employees\",\n    ylabel=\"Number of Applications\",\n    kde=True,\n    bins=60,\n)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"##### Observations\n- There is a large variation in the number of employees of the employers.\n- The distribution is highly right-skewed.\n- Not all the detected outliers per 1.5-IQR rule shall be treated as outliers, because, in 2016, there existed employers in the United States that actually had hundreds of thousands of employees. Here, per the shown distribution, a cut-off value of 450000 is considered for the number of employees.","metadata":{}},{"cell_type":"markdown","source":"# b) Bivariate Analysis\nSince the ultimate goal of this project is producing models to predict employment visa certification, the **focus** of the bivariate analyses will be on the effects of different independent variables on the target variable, i.e., `case_status`.","metadata":{}},{"cell_type":"markdown","source":"#### User-Defined Functions for Bivariate Plots","metadata":{}},{"cell_type":"code","source":"# User-defined function to plot a stacked barplot\ndef stacked_barplot(data, predictor, target, xlabel, ylabel):\n    \"\"\"\n    Print the category counts and plot a stacked bar chart\n\n    data: dataframe\n    predictor: independent variable\n    target: target variable\n    xlabel: label of x-axis\n    ylabel: label of y-axis\n    \"\"\"\n\n    count = data[predictor].nunique()\n    sorter = data[target].value_counts().index[-1]\n    tab1 = pd.crosstab(data[predictor], data[target], margins=True).sort_values(\n        by=sorter, ascending=False\n    )\n    print(tab1)\n    print(\"-\" * 120)\n    tab = pd.crosstab(data[predictor], data[target], normalize=\"index\").sort_values(\n        by=sorter, ascending=False\n    )\n    tab.plot(kind=\"bar\", stacked=True, figsize=(count + 2, 4))\n\n    plt.legend(loc=\"upper left\", bbox_to_anchor=(1, 1), fontsize=16)\n    plt.xlabel(xlabel, fontsize=16)\n    plt.ylabel(ylabel, fontsize=16)\n\n    plt.xticks(fontsize=14)\n    plt.yticks(fontsize=14)\n\n    plt.show()","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# User-defined function to plot distributions w.r.t. target\ndef distribution_plot_wrt_target(data, predictor, target, plabel, tlabel):\n    \"\"\"\n    Print the category counts and plot a stacked bar chart\n\n    data: dataframe\n    predictor: independent variable\n    target: target variable\n    plabel: label of predictor axes\n    tlabel: label of target axes\n    \"\"\"\n\n    fig, axs = plt.subplots(2, 2, figsize=(12, 10))\n\n    target_uniq = data[target].unique()\n\n    sns.histplot(\n        data=data[data[target] == target_uniq[0]],\n        x=predictor,\n        kde=True,\n        ax=axs[0, 0],\n        color=\"teal\",\n        stat=\"density\",\n    )\n    axs[0, 0].set_title(\"Distribution of predictor for target = \" + str(target_uniq[0]))\n    axs[0, 0].set_xlabel(plabel, fontsize=16)\n    axs[0, 0].set_ylabel(\"Density\", fontsize=16)\n\n    sns.histplot(\n        data=data[data[target] == target_uniq[1]],\n        x=predictor,\n        kde=True,\n        ax=axs[0, 1],\n        color=\"orange\",\n        stat=\"density\",\n    )\n    axs[0, 1].set_title(\"Distribution of predictor for target = \" + str(target_uniq[1]))\n    axs[0, 1].set_xlabel(plabel, fontsize=16)\n    axs[0, 1].set_ylabel(\"Density\", fontsize=16)\n\n    sns.boxplot(data=data, x=target, y=predictor, ax=axs[1, 0], palette=\"gist_rainbow\")\n    axs[1, 0].set_title(\"Boxplot w.r.t target\")\n    axs[1, 0].set_xlabel(tlabel, fontsize=16)\n    axs[1, 0].set_ylabel(plabel, fontsize=16)\n\n    sns.boxplot(\n        data=data,\n        x=target,\n        y=predictor,\n        ax=axs[1, 1],\n        showfliers=False,\n        palette=\"gist_rainbow\",\n    )\n    axs[1, 1].set_title(\"Boxplot (without outliers) w.r.t target\")\n    axs[1, 1].set_xlabel(tlabel, fontsize=16)\n    axs[1, 1].set_ylabel(plabel, fontsize=16)\n\n    plt.tight_layout()\n    plt.show()","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"#### Case Status vs. Continent of Origin\n*Leading Question:* How does the visa status vary across different continents? ","metadata":{}},{"cell_type":"code","source":"# Use user-defined function stacked_barplot() to examine case certification likelihoods vs continent of origin\nstacked_barplot(\n    data=df_0,\n    predictor=\"continent\",\n    target=\"case_status\",\n    xlabel=\"Continent of Origin\",\n    ylabel=\"Fraction of Applications\",\n)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"##### Observations\n- Among different continents, *Europe* has the highest work visa certification rate (79%).\n- The lowest work visa certification rate belongs to *South America* (58%).","metadata":{}},{"cell_type":"markdown","source":"#### Case Status vs. Education Level\n*Leading Question:* Those with higher education may want to travel abroad for a well-paid job. Does education play a role in Visa certification?","metadata":{}},{"cell_type":"code","source":"# Use user-defined function stacked_barplot() to examine case certification likelihoods vs education level\nstacked_barplot(\n    data=df_0,\n    predictor=\"education_of_employee\",\n    target=\"case_status\",\n    xlabel=\"Education Level\",\n    ylabel=\"Fraction of Applications\",\n)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"##### Observations\n- It is clear that the higher the education level of an applicants is, the more their chances of visa certification are.\n- More specifically, while the visa certification likelihood of the applicants of a *doctorate* degree is 87%, this likelihood is only 34% for the applicants of *high school* education.","metadata":{}},{"cell_type":"markdown","source":"#### Case Status vs. Job Experience\n*Leading Question:* Experienced professionals might look abroad for opportunities to improve their lifestyles and career development. Does work experience influence visa status? ","metadata":{}},{"cell_type":"code","source":"# Use user-defined function stacked_barplot() to examine case certification likelihoods vs job experience\nstacked_barplot(\n    data=df_0,\n    predictor=\"has_job_experience\",\n    target=\"case_status\",\n    xlabel=\"Job Experience\",\n    ylabel=\"Fraction of Applications\",\n)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"##### Observations\n- Having job experience is found to have a positive effect on the visa certification likelihood.\n- More specifically, about 74% of the experienced applicants are granted visas, while this percentages is only 56% for the inexperienced applicants.","metadata":{}},{"cell_type":"markdown","source":"#### Case Status vs. Job Training Requirement","metadata":{}},{"cell_type":"code","source":"# Use user-defined function stacked_barplot() to examine case certification likelihoods vs training requirement\nstacked_barplot(\n    data=df_0,\n    predictor=\"requires_job_training\",\n    target=\"case_status\",\n    xlabel=\"Training Requirement\",\n    ylabel=\"Fraction of Applications\",\n)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"##### Observations\n- The visa certification likelihood is found nearly unaffected by the job training requirement.","metadata":{}},{"cell_type":"markdown","source":"#### Case Status vs. Employer Region","metadata":{}},{"cell_type":"code","source":"# Use user-defined function stacked_barplot() to examine case certification likelihoods vs employer region\nstacked_barplot(\n    data=df_0,\n    predictor=\"region_of_employment\",\n    target=\"case_status\",\n    xlabel=\"Employer Region\",\n    ylabel=\"Fraction of Applications\",\n)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"##### Observations\n- It appears that the visa applications filed by the employers within the *Midwest* region have the highest probability (~76%) of certification.\n- The employers located in the *Northeast*, *West*, and *Island* regions have lower chances (60-63%) of visa certification.","metadata":{}},{"cell_type":"markdown","source":"#### Case Status vs. Position Type","metadata":{}},{"cell_type":"code","source":"# Use user-defined function stacked_barplot() to examine case certification likelihoods vs position type\nstacked_barplot(\n    data=df_0,\n    predictor=\"full_time_position\",\n    target=\"case_status\",\n    xlabel=\"Full-Time Position\",\n    ylabel=\"Fraction of Applications\",\n)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"##### Observations\n- Visa certification seems to be unaffected by whether a position is full-time or part-time.","metadata":{}},{"cell_type":"markdown","source":"#### Case Status vs. Wage Unit\n*Leading Question:* In the United States, employees are paid at different intervals. Which pay unit is most likely to be certified for a visa? ","metadata":{}},{"cell_type":"code","source":"# Use user-defined function stacked_barplot() to examine case certification likelihoods vs unit of prevailing wage\nstacked_barplot(\n    data=df_0,\n    predictor=\"unit_of_wage\",\n    target=\"case_status\",\n    xlabel=\"Wage Unit\",\n    ylabel=\"Fraction of Applications\",\n)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"##### Observations\n- Those applicants whose wage unit is *year* are more likely than other applicants to be certified for a visa (~70% likelihood).\n- The applicants who are paid by hour are the least likely to be certified for a visa (~35% likelihood). This could be predicted, because hourly jobs are usually less important for the growth of the United States and they could be done by normal American workers.","metadata":{}},{"cell_type":"markdown","source":"#### Case Status vs. Number of Employees","metadata":{}},{"cell_type":"code","source":"# Use user-defined function distribution_plot_wrt_target() to examine case certification likelihoods across data categories\ndistribution_plot_wrt_target(\n    data=df_0,\n    predictor=\"no_of_employees\",\n    target=\"case_status\",\n    plabel=\"Number of Employees\",\n    tlabel=\"Case Status\",\n)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"##### Observations\n- A very small difference is observed between the distributions of the employer's number of employees for those applications that are denied and those that are certified. As a result, it seems that the number of employees has insignificant effect on the likelihood of visa certification.","metadata":{}},{"cell_type":"markdown","source":"#### Training Requirement vs. Job Experience","metadata":{}},{"cell_type":"code","source":"# Use seaborn heatmap to compare number of applications pivoted on job experience and training requirement\n\n# Create a count pivot table with respect to columns has_job_experience and requires_job_training\npt = df_0.pivot_table(\n    values=\"case_status\",\n    index=\"has_job_experience\",\n    columns=\"requires_job_training\",\n    aggfunc=\"count\",\n)\n\n# Plot a heatmap\nplt.figure(figsize=(6, 4))\nsns.heatmap(pt, square=True, annot=True, fmt=\"g\")\nplt.ylabel(\"Job Experience\", fontsize=15)\nplt.xlabel(\"Training Requirement\", fontsize=15)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"##### Observations\n- Reasonably, a higher percentage of the applicants who have no job experience require job training than the applicants who have job experience (16% vs. ~9%).","metadata":{}},{"cell_type":"markdown","source":"#### Job Training Requirement vs. Continent","metadata":{}},{"cell_type":"code","source":"# Use user-defined function stacked_barplot() to examine job training requirement vs continent of origin of applicants\nstacked_barplot(\n    data=df_0,\n    predictor=\"continent\",\n    target=\"requires_job_training\",\n    xlabel=\"Continent of Origin\",\n    ylabel=\"Fraction of Applications\",\n)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"##### Observations\n- Among the applicants from different continents, a smaller ratio of those from *Africa* and *North America* need training than those from other continents.\n- The highest ratio of the applicants who need training belongs to those from *Europe*.","metadata":{}},{"cell_type":"markdown","source":"# Data Preprocessing","metadata":{"id":"alleged-spirituality"}},{"cell_type":"code","source":"# Create a copy of data frame before preprocessing\ndf_1 = df_0.copy()","metadata":{"id":"increasing-louisiana"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# a) Treatment of Missing Values\nBased on the initial evaluations, no values were missing in any of the columns. However, there were rows with unrealistic non-positive (<0) values of `no_of_employees`. To address this problem, these values are replaced with the median of `no_of_employees`.","metadata":{}},{"cell_type":"code","source":"# Identify rows with non-positive no_of_employees\nneg_employee_no_rows = df_1.no_of_employees <= 0\n\n# Print number of rows with non-positive no_of_employees\nprint(\n    f\"There are {neg_employee_no_rows.sum()} rows with non-positive number of employees.\"\n)\n\n# Replace negative values in column no_of_employees with its median\ndf_1.loc[neg_employee_no_rows, \"no_of_employees\"] = df_1.no_of_employees.median()\n\n# Double-check minimum value of no_of_employees\nprint(f\"The new minimum number of employees is {df_1.no_of_employees.min()}.\")","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# b) Feature Engineering\nThe feature `yr_of_estab` is converted to `yrs_snc_estab`, containing the years since establishment. Also, to make the prevailing wages (in the column `prevailing_wage`) interpretable across the rows, they are all transformed into an *equivalent* hourly wage and are saved in a new column, `hourly_wage`. The columns `yr_of_estab` and `prevailing_wage` are dropped subsequently.","metadata":{}},{"cell_type":"code","source":"# Add a new column, yrs_snc_estab, including years since establishment - final year is 2016, when data is gathered\ndf_1[\"yrs_snc_estab\"] = 2016 - df_1.yr_of_estab\n\n# Drom yr_of_estab\ndf_1.drop(\"yr_of_estab\", axis=1, inplace=True)\n\n# Create a column including equivalent hourly wages - it is assumed that:\n# A year includes 2080 work-hours\n# A month includes 173 work-hours\n# A week includes 40 work-hours\ndf_1[\"hourly_wage\"] = df_1[\"prevailing_wage\"]\ndf_1.loc[df_1.unit_of_wage == \"Year\", \"hourly_wage\"] = (\n    df_1.loc[df_1.unit_of_wage == \"Year\", \"hourly_wage\"] / 2080.0\n)\ndf_1.loc[df_1.unit_of_wage == \"Month\", \"hourly_wage\"] = (\n    df_1.loc[df_1.unit_of_wage == \"Month\", \"hourly_wage\"] / 173.0\n)\ndf_1.loc[df_1.unit_of_wage == \"Week\", \"hourly_wage\"] = (\n    df_1.loc[df_1.unit_of_wage == \"Week\", \"hourly_wage\"] / 40.0\n)\n\n# Drom yr_of_estab\ndf_1.drop(\"prevailing_wage\", axis=1, inplace=True)\n\n# Check sample rows of updated data\ndf_1.sample(10, random_state=1)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Check statistical summary of numeric data in updated data\ndf_1.describe().T","metadata":{"scrolled":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"##### Observations\n- The mean and median values of `yrs_snc_estab` are ~37 and 19 years, respectively. The oldest employer was established 216 years before the data collection.\n- The minimum and maximum values of `hourly_wage` are 0.05 and ~7004 (probably in dollars), respectively, so the variation of this variable is very large. The mean hourly wage is ~95.","metadata":{}},{"cell_type":"markdown","source":"# c) Detection and Treatment of Outliers\n#### Detection of Outliers\nInitially, the 1.5-IQR rule is used to detect *potential* outliers. However, it is noted that all the values detected as outlier by this method are not always outliers.","metadata":{}},{"cell_type":"code","source":"# Create a list of column names including numeric data\nnum_cols = df_1.select_dtypes(include=np.number).columns.tolist()\n\n# Use boxplots with 1.5*IQR whiskers for each numeric variable to detect potential outliers\nplt.figure(figsize=(9, 3))\n\nfor i, variable in enumerate(num_cols):\n    plt.subplot(1, 3, i + 1)\n    plt.boxplot(df_1[variable], whis=1.5)\n    plt.tight_layout()\n    plt.title(variable)\n\nplt.show()","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"##### Observations\n- Given the discussions provided in the initial EDA section, not all the outliers detected based on the 1.5-IQR rule are actual outliers. Here, merely to remove very large infrequent values, the following maximum cut-off values are considered for the above three variables:\n    - `no_of_employees`: 450000\n    - `yrs_snc_estab`: 200\n    - `hourly_wage`: 4000","metadata":{}},{"cell_type":"markdown","source":"#### Treatment of Outliers\n- The detected upper outliers are replaced with the maximum values of the respective columns in the absence of the outliers.","metadata":{}},{"cell_type":"code","source":"# Replace outliers in no_of_employees\ndf_1.loc[df_1.no_of_employees > 450000, \"no_of_employees\"] = df_1[\n    df_1.no_of_employees <= 450000\n].no_of_employees.max()\n\n# Replace outliers in yrs_snc_estab\ndf_1.loc[df_1.yrs_snc_estab > 200, \"yrs_snc_estab\"] = df_1[\n    df_1.yrs_snc_estab <= 200\n].yrs_snc_estab.max()\n\n# Replace outliers in hourly_wage\ndf_1.loc[df_1.hourly_wage > 4000, \"hourly_wage\"] = df_1[\n    df_1.hourly_wage <= 4000\n].hourly_wage.max()\n\n# Use boxplots to check distributions again\nplt.figure(figsize=(9, 3))\n\nfor i, variable in enumerate(num_cols):\n    plt.subplot(1, 3, i + 1)\n    plt.boxplot(df_1[variable], whis=1.5)\n    plt.tight_layout()\n    plt.title(variable)\n\nplt.show()","metadata":{"scrolled":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Secondary EDA\nThe focus of the secondary EDA is on the new variables created in the section Data Preprocessing, while correlation coefficients between the final numeric variables are also examined.","metadata":{"id":"difficult-union"}},{"cell_type":"markdown","source":"### Univariate Analysis","metadata":{}},{"cell_type":"markdown","source":"#### Years Since Establishment","metadata":{}},{"cell_type":"code","source":"# Use user-defined function histogram_boxplot() to examine distribution of data\nhistogram_boxplot(\n    data=df_1,\n    feature=\"yrs_snc_estab\",\n    xlabel=\"Years Since Establishment\",\n    ylabel=\"Number of Applications\",\n    kde=True,\n    bins=40,\n)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"##### Observations\n- The distribution is quite right-skewed and the majority of the employers are less than 40 years old.\n- As mentioned in the previous section on the treatment of outliers, the detected outliers per 1.5-IQR rule are not actually outliers.","metadata":{}},{"cell_type":"markdown","source":"#### Hourly Wage","metadata":{}},{"cell_type":"code","source":"# Use user-defined function histogram_boxplot() to examine distribution of data\nhistogram_boxplot(\n    data=df_1,\n    feature=\"hourly_wage\",\n    xlabel=\"Hourly Wage\",\n    ylabel=\"Number of Applications\",\n    kde=True,\n    bins=70,\n)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"##### Observations\n- The distribution of the computed equivalent hourly wage is highly right-skewed and the majority of the applications are for the positions with less than 100 (dollars) of equivalent hourly wage.\n- Since there are certain positions in certain industries that are paid millions of dollars per year, the detected outliers are not actual outliers.","metadata":{}},{"cell_type":"markdown","source":"### Bivariate Analysis","metadata":{"id":"interested-talent"}},{"cell_type":"markdown","source":"#### Linear Correlation Coefficients\nThe linear correlation coefficients are only determined between the numeric variables, i.e., `no_of_employees`, `yrs_snc_estab`, and `hourly_wage`.","metadata":{}},{"cell_type":"code","source":"# Create a list of column names including numeric data\nnum_cols = df_1.select_dtypes(include=np.number).columns.tolist()\n\n# Compute correlation coefficients\nrhos = df_1[num_cols].corr()\n\n# Plot heatmap\nplt.figure(figsize=(8, 6))  # set figure size\n\np = sns.heatmap(\n    rhos,\n    annot=True,\n    square=True,\n    vmin=-1,\n    vmax=1,\n    fmt=\".2f\",\n    cmap=\"Spectral\",\n)  # create heatmap\n\np.set_title(\"Correlation Coefficients\", fontsize=16)\n# set chart's title","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"##### Observations\n- Negligible linear correlation is observed between the numeric variables.","metadata":{}},{"cell_type":"markdown","source":"#### Pairplot","metadata":{}},{"cell_type":"code","source":"# Add case_status to list of column names including numeric data\nnum_cols = num_cols + [\"case_status\"]\n\n# Create a pairplot to see distributions of and relationships between variations of numeric data\nsns.pairplot(data=df_1[num_cols], hue=\"case_status\", diag_kind=\"kde\", aspect=1)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"##### Observations\n- No linear correlation is observed between the numeric variables.\n- It is hard to identify the effects of the above variables on the visa certification likelihood.","metadata":{}},{"cell_type":"markdown","source":"#### Case Status vs. Hourly Wage\n*Leading Question:* The US government has established a prevailing wage to protect local talent and foreign workers. How does the visa status change with the prevailing wage?","metadata":{}},{"cell_type":"code","source":"# Use user-defined function distribution_plot_wrt_target() to examine case certification likelihoods across data categories\ndistribution_plot_wrt_target(\n    data=df_1,\n    predictor=\"hourly_wage\",\n    target=\"case_status\",\n    plabel=\"Hourly Wage\",\n    tlabel=\"Case Status\",\n)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"##### Observations\n- It appears that a decrease in the equivalent hourly wage would lead to an increase in the likelihood of visa certification. This could be justified by the fact that the jobs that are paid higher could be more easily filled by American workers, making the emplyment of aliens unjustifiable.","metadata":{}},{"cell_type":"markdown","source":"#### Hourly Wage vs. Education Level","metadata":{}},{"cell_type":"code","source":"# Use seaborn boxplot to compare distributions of hourly wage for different education levels without outliers\nplt.figure(figsize=(6, 4))\n# set figure size\nsns.boxplot(\n    data=df_1,\n    y=\"education_of_employee\",\n    x=\"hourly_wage\",\n    showmeans=True,\n    showfliers=False,\n    palette=\"Set2\",\n)  # create box plot\n\n# set axis labels\nplt.xlabel(\"Hourly Wage\", fontsize=16)\nplt.ylabel(\"Education Level\", fontsize=16)\n\n# set font size for axis ticks\nplt.xticks(fontsize=14)\nplt.yticks(fontsize=14)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"##### Observations\n- Surprisingly, on average, the employees of less education (e.g., high school and bachelor's degree) seem to be paid more in terms of equivalent hourly wage than the employees of higher education, particularly, those of a doctorate degree.","metadata":{}},{"cell_type":"markdown","source":"#### Hourly Wage vs. Job Experience","metadata":{}},{"cell_type":"code","source":"# Use seaborn boxplot to compare distributions of hourly wage with respect to job experience\nplt.figure(figsize=(6, 2))\n# set figure size\nsns.boxplot(\n    data=df_1,\n    y=\"has_job_experience\",\n    x=\"hourly_wage\",\n    showmeans=True,\n    showfliers=False,\n    palette=\"Set2\",\n)  # create box plot\n\n# set axis labels\nplt.xlabel(\"Hourly Wage\", fontsize=16)\nplt.ylabel(\"Job Experience\", fontsize=16)\n\n# set font size for axis ticks\nplt.xticks(fontsize=14)\nplt.yticks(fontsize=14)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"##### Observations\n- Surprisingly, on average, those employees that have job experience seem to receive lower equivalent hourly wage than those who have no job experience.","metadata":{}},{"cell_type":"markdown","source":"#### Hourly Wage vs. Job Training","metadata":{}},{"cell_type":"code","source":"# Use seaborn boxplot to compare distributions of hourly wage with respect to job training requirement\nplt.figure(figsize=(6, 2))\n# set figure size\nsns.boxplot(\n    data=df_1,\n    y=\"requires_job_training\",\n    x=\"hourly_wage\",\n    showmeans=True,\n    showfliers=False,\n    palette=\"Set2\",\n)  # create box plot\n\n# set axis labels\nplt.xlabel(\"Hourly Wage\", fontsize=16)\nplt.ylabel(\"Training Requirement\", fontsize=16)\n\n# set font size for axis ticks\nplt.xticks(fontsize=14)\nplt.yticks(fontsize=14)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"##### Observations\n- On average, the equivalent hourly wage of the applicants who do not require training is higher than those who require training. ","metadata":{}},{"cell_type":"markdown","source":"#### Case Status vs. Years Since Establishment","metadata":{}},{"cell_type":"code","source":"# Use user-defined function distribution_plot_wrt_target() to examine case certification likelihoods across data categories\ndistribution_plot_wrt_target(\n    data=df_1,\n    predictor=\"yrs_snc_estab\",\n    target=\"case_status\",\n    plabel=\"Years Since Establishment\",\n    tlabel=\"Case Status\",\n)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"##### Observations\n- A very small difference is observed between the distributions of the employer's age for those applications that are denied and those that are certified. As a result, it seems that the number of years since establishment has insignificant effect on the likelihood of visa certification.","metadata":{}},{"cell_type":"markdown","source":"#### Number of Employees vs. Years Since Establishment","metadata":{}},{"cell_type":"code","source":"# Use seaborn jointplot to compare distributions of number of employees vs years since employer's establishment\nplt.figure(figsize=(4, 4))\n# set figure size\nsns.jointplot(data=df_1, x=\"yrs_snc_estab\", y=\"no_of_employees\", kind=\"hex\", bins=10)\n# create joint plot\n\nplt.xlabel(\"Years Since Establishment\", fontsize=16)\n# set x-axis label\nplt.ylabel(\"Number of Employees\", fontsize=16)\n# set y-axis label","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"##### Observations\n- Older employers seem to tend to have slightly smaller number of employees compared to the younger employers.","metadata":{}},{"cell_type":"markdown","source":"# Data Preparation for Modeling","metadata":{}},{"cell_type":"markdown","source":"# a) Encoding Categorical Data\nEncoding the values in the columns `has_job_experience`, `requires_job_training`, `full_time_position`, `case_status` and `education_of_employee`.","metadata":{}},{"cell_type":"code","source":"# has_job_experience, requires_job_training, and full_time_position:\n# Replace 'Y' with 1 and 'N' with 0\ndf_1.has_job_experience = df_1.has_job_experience.apply(lambda x: 1 if x == \"Y\" else 0)\ndf_1.requires_job_training = df_1.requires_job_training.apply(\n    lambda x: 1 if x == \"Y\" else 0\n)\ndf_1.full_time_position = df_1.full_time_position.apply(lambda x: 1 if x == \"Y\" else 0)\n\n# case_status:\n# Replace 'Certified' with 1 and 'Denied' with 0\ndf_1.case_status = df_1.case_status.apply(lambda x: 1 if x == \"Certified\" else 0)\n\n# education_of_employee:\n# Replace 'High School' with 1, 'Bachelor's' with 2, 'Master's' with 3, and 'Doctarate' with 4\ndf_1.education_of_employee = df_1.education_of_employee.apply(\n    lambda x: 1\n    if x == \"High School\"\n    else (2 if x == \"Bachelor's\" else (3 if x == \"Master's\" else 4))\n)\n\n# Check updated sample rows\ndf_1.sample(10, random_state=1)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Separation of Dependent and Independent Variables","metadata":{}},{"cell_type":"code","source":"# Create a data frame with only independent variables\nX = df_1.drop([\"case_status\"], axis=1)\n\n# Create a series with only dependent variable\nY = df_1.case_status\n\n# Print some rows of X and Y data frames to check them\nprint(\"Independent Variables\\n\", \"=\" * 80, \"\\n\", X.sample(5, random_state=1))\nprint(\"\\n\\nDependent Variables\\n\", \"=\" * 80, \"\\n\", Y.sample(5, random_state=1))","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# b) Creating Dummy Variables\nCreate dummy variables for the categorical columns, i.e., `unit_of_wage`, `continent`, and `region_of_employment`.","metadata":{}},{"cell_type":"code","source":"# Use pandas function get_dummies to create dummy variables and drop their first one\nX = pd.get_dummies(X, drop_first=True)\n\n# Check updated independent variables data frame\nX.sample(5, random_state=1)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# c) Splitting Data into Training and Test Sets","metadata":{}},{"cell_type":"code","source":"# Use function train_test_split to create training and testing data sets for both dependnet and independent variables\nX_train, X_test, Y_train, Y_test = train_test_split(\n    X, Y, test_size=0.3, random_state=1, stratify=Y\n)\n\n# Check number of rows in each data set\nprint(\"Number of rows in training data set =\", X_train.shape[0])\nprint(\"\\nNumber of rows in test data set =\", X_test.shape[0])\n\n# Show percentage of number of rows in each data set\nprint(\"\\nPercentage of classes in training set:\")\nprint(Y_train.value_counts(normalize=True))\nprint(\"\\nPercentage of classes in test set:\")\nprint(Y_test.value_counts(normalize=True))","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Building Prediction Models\n# a) Evaluation Criterion\n#### Possible Errors\n- Prediction of visa certification while the visa will actually be denied, i.e., false positive.\n- Prediction of visa denial while the visa will actually be certified, i.e., false negative.\n\n#### More Important Error\n\nA false positive would lead to the waste of the OFLC's time and staff resources, while a false negative would prevent a qualified applicant who could fill essential jobs in the United States from receiving work visa. Therefore, it appears that both errors could be equally important for the OFLC to be minimized.\n\n#### Optimal Performance Measure\n\nGiven the foregoing, to minimize both the false positive and false negative errors simoltaneously, it is decided that ***F1-score*** could be the optimal performance measure for the models built subsequently. That is, the best model would maximize F1-score, while it would not be overfitting or underfitting the training data.","metadata":{}},{"cell_type":"markdown","source":"#### User-Defined Functions for Model Performance Evaluation ","metadata":{}},{"cell_type":"code","source":"# User-defined function to compute different performance metrics to evaluate a classification model built using sklearn\ndef get_metrics_score(model, flag=True):\n    \"\"\"\n    model: classifier to predict values of Y\n    \"\"\"\n\n    # Predict Y using independent variables\n    pred_train = model.predict(X_train)\n    pred_test = model.predict(X_test)\n\n    # Compute performance metrics\n    train_acc = accuracy_score(Y_train, pred_train)  # accuracy\n    test_acc = accuracy_score(Y_test, pred_test)\n\n    train_recall = recall_score(Y_train, pred_train)  # recall\n    test_recall = recall_score(Y_test, pred_test)\n\n    train_precision = precision_score(Y_train, pred_train)  # precision\n    test_precision = precision_score(Y_test, pred_test)\n\n    train_f1 = f1_score(Y_train, pred_train)  # f1-score\n    test_f1 = f1_score(Y_test, pred_test)\n\n    # Create a dataframe of metrics\n    df_perf = pd.DataFrame(\n        {\n            \"Accuracy\": [train_acc, test_acc],\n            \"Recall\": [train_recall, test_recall],\n            \"Precision\": [train_precision, test_precision],\n            \"F1\": [train_f1, test_f1],\n        },\n        index=[\"Training\", \"Test\"],\n    )\n\n    return df_perf","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# User-defined function to plot the confusion_matrix of a classification model built using sklearn based on test set\ndef make_confusion_matrix(model):\n    \"\"\"\n    model: classifier to predict values of Y\n    \"\"\"\n    Y_pred = model.predict(X_test)\n    cm = confusion_matrix(Y_test, Y_pred)\n    labels = np.asarray(\n        [\n            [\"{0:0.0f}\".format(item) + \"\\n{0:.2%}\".format(item / cm.flatten().sum())]\n            for item in cm.flatten()\n        ]\n    ).reshape(2, 2)\n\n    plt.figure(figsize=(6, 4))\n    sns.heatmap(cm, annot=labels, fmt=\"\")\n    plt.title(\"Test Set's Confusion Matrix\", fontsize=16)\n    plt.ylabel(\"Actual Label\", fontsize=15)\n    plt.xlabel(\"Predicted Label\", fontsize=15)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Decision Tree Classifier","metadata":{}},{"cell_type":"code","source":"# Use function DecisionTreeClassifier from sklearn to build model - consider `gini` criterion to split data at nodes\ndcsn_tree = DecisionTreeClassifier(criterion=\"gini\", random_state=1)\ndcsn_tree.fit(X_train, Y_train)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Create confusion matrix based on test data set\nmake_confusion_matrix(dcsn_tree)\n\n# Check performance of model on both training and test data sets\nperf_dcsn_tree = get_metrics_score(dcsn_tree)\nperf_dcsn_tree","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"##### Observations\n- The initial decision tree model works very well for the training data set - all performance metrics, i.e., accuracy , recall, precision, and F1-score are 1.00. \n- However, the performance is not as good for the test set (F1-score is 0.74), implying overfitting. As a result, there is need for hyperparameter tuning through grid search.","metadata":{}},{"cell_type":"markdown","source":"### Decision Tree Classifier with Hyperparameter Tuning","metadata":{}},{"cell_type":"code","source":"# Choose type of classifier\ntnd_dcsn_tree = DecisionTreeClassifier(random_state=1)\n\n# Form grid of parameters to search in\ngrid_para = {\n    \"class_weight\": [\"balanced\", None],\n    \"max_depth\": np.arange(2, 21, 2),\n    \"max_leaf_nodes\": np.arange(2, 21, 2),\n    \"min_samples_split\": [100, 200, 400, 800],\n    \"min_impurity_decrease\": [0.0001, 0.001, 0.01],\n}\n\n# Set type of score used to evaluate performance throughout search\nscorer = make_scorer(f1_score)\n\n# Run GridSearch\ngrid_obj = GridSearchCV(tnd_dcsn_tree, grid_para, scoring=scorer, cv=5)\ngrid_obj = grid_obj.fit(X_train, Y_train)\n\n# Set classifer to best combination of parameters\ntnd_dcsn_tree = grid_obj.best_estimator_\n\n# Fit best decision tree to training data\ntnd_dcsn_tree.fit(X_train, Y_train)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Create confusion matrix based on test data set\nmake_confusion_matrix(tnd_dcsn_tree)\n\n# Check performance of model on both training and test data sets\nperf_tnd_dcsn_tree = get_metrics_score(tnd_dcsn_tree)\nperf_tnd_dcsn_tree","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"##### Observations\n- The tuned decision tree model has a better overall performance than the initial decision tree model. Specifically, all its metrics are almost equal for both training and test data sets, indicating that the model is not overfitting anymore.\n- The F1-score for the test set has been increased from 0.74 for the initial model to 0.82 for the tuned model.","metadata":{}},{"cell_type":"code","source":"# Create a list of column names - features of tree\ncol_names = list(X.columns)\n\n# Check importances of various features of tuned tree\nimportances = tnd_dcsn_tree.feature_importances_\nindices = np.argsort(importances)\n\nplt.figure(figsize=(6, 0.5 * len(col_names)))\nplt.barh(range(len(indices)), importances[indices], color=\"violet\", align=\"center\")\nplt.yticks(range(len(indices)), [col_names[i] for i in indices])\nplt.xlabel(\"Relative Importance\", fontsize=15)\nplt.ylabel(\"Feature\", fontsize=15)\nplt.show()","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"##### Observations\n- The top four independent variables of importance in the tuned decision tree model are `education_of_employee`, `has_job_experience`, `unit_of_wage_Year`, and `continent_Europe`.","metadata":{}},{"cell_type":"markdown","source":"# Bagging Classifier","metadata":{}},{"cell_type":"code","source":"# Use function BaggingClassifier from sklearn to build model\nbagging = BaggingClassifier(random_state=1)\nbagging.fit(X_train, Y_train)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Create confusion matrix based on test data set\nmake_confusion_matrix(bagging)\n\n# Check performance of model on both training and test data sets\nperf_bagging = get_metrics_score(bagging)\nperf_bagging","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"##### Observations\n- Compared to the initial decision tree model (not tuned), this model has slightly better performance on the test data set. \n- However, considering the very high performance metrics for the training data set, it is clear that the model is overfitting and needs hyperparameter tuning.","metadata":{}},{"cell_type":"markdown","source":"### Bagging Classifier with Hyperparameter Tuning","metadata":{}},{"cell_type":"code","source":"# Choose type of classifier\ntnd_bagging = BaggingClassifier(random_state=1)\n\n# Form grid of parameters to search in\ngrid_para = {\n    \"max_samples\": [0.7, 0.8, 0.9, 1.0],\n    \"max_features\": [0.7, 0.8, 0.9, 1.0],\n    \"n_estimators\": np.arange(20, 101, 20),\n}\n\n# Set type of score used to evaluate performance throughout search\nscorer = make_scorer(f1_score)\n\n# Run GridSearch\ngrid_obj = GridSearchCV(tnd_bagging, grid_para, scoring=scorer, cv=5)\ngrid_obj = grid_obj.fit(X_train, Y_train)\n\n# Set classifer to best combination of parameters\ntnd_bagging = grid_obj.best_estimator_\n\n# Fit best decision tree to training data\ntnd_bagging.fit(X_train, Y_train)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Create confusion matrix based on test data set\nmake_confusion_matrix(tnd_bagging)\n\n# Check performance of model on both training and test data sets\nperf_tnd_bagging = get_metrics_score(tnd_bagging)\nperf_tnd_bagging","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"##### Observations\n- As seen, the model seems to still overfit the training data.\n- On the test data set, the tuned model's performance has been slightly improved compared to the initial bagging model - the F1-score has been increased from 0.77 for the initial model to 0.81 for the tuned model.","metadata":{}},{"cell_type":"markdown","source":"# Random Forest Classifier","metadata":{}},{"cell_type":"code","source":"# Use function RandomForestClassifier from sklearn to build model\nrndm_frst = RandomForestClassifier(random_state=1)\nrndm_frst.fit(X_train, Y_train)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Create confusion matrix based on test data set\nmake_confusion_matrix(rndm_frst)\n\n# Check performance of model on both training and test data sets\nperf_rndm_frst = get_metrics_score(rndm_frst)\nperf_rndm_frst","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"##### Observations\n- Compared to the initial decision tree model (not tuned), this model also has slightly better performance on the test data set. \n- However, the metrics all equal 1.00 for the training data set, indicating overfitting. As a result, there is need for hyperparameter tuning.","metadata":{}},{"cell_type":"markdown","source":"### Random Forest Classifier with Hyperparameter Tuning","metadata":{}},{"cell_type":"code","source":"#### Choose type of classifier\n# Set oob_score as True to consider out-of-bag samples to estimate generalization score\ntnd_rndm_frst = RandomForestClassifier(oob_score=True, random_state=1)\n\n# Form grid of parameters to search in\ngrid_para = {\n    \"class_weight\": [\"balanced\", None],\n    \"max_samples\": [0.7, 0.8, 0.9, 1.0],\n    \"max_depth\": np.arange(1, 5, 1),\n    \"max_features\": [\"sqrt\", \"log2\"],\n    \"min_samples_split\": [100, 200, 400, 800],\n    \"n_estimators\": np.arange(20, 110, 20),\n}\n\n# Set type of score used to evaluate performance throughout search\nscorer = make_scorer(f1_score)\n\n# Run GridSearch\ngrid_obj = GridSearchCV(tnd_rndm_frst, grid_para, scoring=scorer, cv=5)\ngrid_obj = grid_obj.fit(X_train, Y_train)\n\n# Set classifer to best combination of parameters\ntnd_rndm_frst = grid_obj.best_estimator_\n\n# Fit best decision tree to training data\ntnd_rndm_frst.fit(X_train, Y_train)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Create confusion matrix based on test data set\nmake_confusion_matrix(tnd_rndm_frst)\n\n# Check performance of model on both training and test data sets\nperf_tnd_rndm_frst = get_metrics_score(tnd_rndm_frst)\nperf_tnd_rndm_frst","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"##### Observations\n- The performance metrics are very close for the training and test data sets, showing that the model is not overfitting anymore.\n- Compared to the initial random forest model (before tuning), on the test data, precision has decreased, but recall and F1-score have been increased.","metadata":{}},{"cell_type":"code","source":"# Check importances of various features of tuned random forest classifier\nimportances = tnd_rndm_frst.feature_importances_\nindices = np.argsort(importances)\n\nplt.figure(figsize=(6, 0.5 * len(col_names)))\nplt.barh(range(len(indices)), importances[indices], color=\"violet\", align=\"center\")\nplt.yticks(range(len(indices)), [col_names[i] for i in indices])\nplt.xlabel(\"Relative Importance\", fontsize=15)\nplt.ylabel(\"Feature\", fontsize=15)\nplt.show()","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"##### Observations\n- The top four independent features of importance in the tuned random forest model are `education_of_employee`, `has_job_experience`, `unit_of_wage_Year`, and `hourly_wage`. Compared to the imprtant features in the tuned decision tree, only `continent_Europe` has been replaced with `hourly_wage`.","metadata":{}},{"cell_type":"markdown","source":"# AdaBoost Classifier","metadata":{}},{"cell_type":"code","source":"# Use function AdaBoostClassifier from sklearn to build model\nada_boost = AdaBoostClassifier(random_state=1)\nada_boost.fit(X_train, Y_train)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Create confusion matrix based on test data set\nmake_confusion_matrix(ada_boost)\n\n# Check performance of model on both training and test data sets\nperf_ada_boost = get_metrics_score(ada_boost)\nperf_ada_boost","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"##### Observations\n- The model seems to already be generalizable, as the performance metrics for the training and test data sets are very close.\n- Yet, a hyperparameter tuning may help to improve the model's performance.","metadata":{}},{"cell_type":"markdown","source":"### AdaBoost Classifier with Hyperparameter Tuning","metadata":{}},{"cell_type":"code","source":"# Choose type of classifier\ntnd_ada_boost = AdaBoostClassifier(random_state=1)\n\n# Form grid of parameters to search in\ngrid_para = {\n    \"base_estimator\": [\n        DecisionTreeClassifier(max_depth=1),\n        DecisionTreeClassifier(max_depth=2),\n        DecisionTreeClassifier(max_depth=3),\n    ],\n    \"n_estimators\": np.arange(20, 110, 20),\n    \"learning_rate\": np.arange(0.2, 1.1, 0.2),\n}\n\n# Set type of score used to evaluate performance throughout search\nscorer = make_scorer(f1_score)\n\n# Run GridSearch\ngrid_obj = GridSearchCV(tnd_ada_boost, grid_para, scoring=scorer, cv=5)\ngrid_obj = grid_obj.fit(X_train, Y_train)\n\n# Set classifer to best combination of parameters\ntnd_ada_boost = grid_obj.best_estimator_\n\n# Fit best decision tree to training data\ntnd_ada_boost.fit(X_train, Y_train)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Create confusion matrix based on test data set\nmake_confusion_matrix(tnd_ada_boost)\n\n# Check performance of model on both training and test data sets\nperf_tnd_ada_boost = get_metrics_score(tnd_ada_boost)\nperf_tnd_ada_boost","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"##### Observations\n- No significant improvement is observed in the model performance after tuning.","metadata":{}},{"cell_type":"code","source":"# Check importances of various features of tuned AdaBoost classifier\nimportances = tnd_ada_boost.feature_importances_\nindices = np.argsort(importances)\n\nplt.figure(figsize=(6, 0.5 * len(col_names)))\nplt.barh(range(len(indices)), importances[indices], color=\"violet\", align=\"center\")\nplt.yticks(range(len(indices)), [col_names[i] for i in indices])\nplt.xlabel(\"Relative Importance\", fontsize=15)\nplt.ylabel(\"Feature\", fontsize=15)\nplt.show()","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"##### Observations\n- The top four independent features of importance in the tuned AdaBoost model are `education_of_employee`, `has_job_experience`, `continent_Europe`, and `unit_of_wage_Year`. ","metadata":{}},{"cell_type":"markdown","source":"# Gradient Boosting Classifier","metadata":{}},{"cell_type":"code","source":"# Use function GradientBoostingClassifier from sklearn to build model\ngrdnt_boost = GradientBoostingClassifier(random_state=1)\ngrdnt_boost.fit(X_train, Y_train)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Create confusion matrix based on test data set\nmake_confusion_matrix(grdnt_boost)\n\n# Check performance of model on both training and test data sets\nperf_grdnt_boost = get_metrics_score(grdnt_boost)\nperf_grdnt_boost","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"##### Observations\n- The model already seems to perform well on both the training and test data sets and does not show overfitting.\n- The F1-score for both training and test data sets is above 0.82, which is quite good.","metadata":{}},{"cell_type":"markdown","source":"### Gradient Boosting Classifier with Hyperparameter Tuning","metadata":{}},{"cell_type":"code","source":"# Choose type of classifier\ntnd_grdnt_boost = GradientBoostingClassifier(\n    init=AdaBoostClassifier(random_state=1), random_state=1\n)\n\n# Form grid of parameters to search in\ngrid_para = {\n    \"subsample\": [0.8, 0.9, 1.0],\n    \"max_features\": [0.8, 0.9, 1.0],\n    \"n_estimators\": np.arange(20, 110, 20),\n    \"learning_rate\": np.arange(0.2, 1.1, 0.2),\n}\n\n# Set type of score used to evaluate performance throughout search\nscorer = make_scorer(f1_score)\n\n# Run GridSearch\ngrid_obj = GridSearchCV(tnd_grdnt_boost, grid_para, scoring=scorer, cv=5)\ngrid_obj = grid_obj.fit(X_train, Y_train)\n\n# Set classifer to best combination of parameters\ntnd_grdnt_boost = grid_obj.best_estimator_\n\n# Fit best decision tree to training data\ntnd_grdnt_boost.fit(X_train, Y_train)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Create confusion matrix based on test data set\nmake_confusion_matrix(tnd_grdnt_boost)\n\n# Check performance of model on both training and test data sets\nperf_tnd_grdnt_boost = get_metrics_score(tnd_grdnt_boost)\nperf_tnd_grdnt_boost","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"##### Observations\n- The hyperparameter tuning barely improves the performance of the gradient boosting model.","metadata":{}},{"cell_type":"code","source":"# Check importances of various features of tuned gradient boosting classifier\nimportances = tnd_grdnt_boost.feature_importances_\nindices = np.argsort(importances)\n\nplt.figure(figsize=(6, 0.5 * len(col_names)))\nplt.barh(range(len(indices)), importances[indices], color=\"violet\", align=\"center\")\nplt.yticks(range(len(indices)), [col_names[i] for i in indices])\nplt.xlabel(\"Relative Importance\", fontsize=15)\nplt.ylabel(\"Feature\", fontsize=15)\nplt.show()","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"##### Observations\n- The top four independent features of importance in the tuned gradient boosting model are `education_of_employee`, `has_job_experience`, `unit_of_wage_Year`, and `continent_Europe`. ","metadata":{}},{"cell_type":"markdown","source":"# XGBoost Classifier","metadata":{}},{"cell_type":"code","source":"# Use function XGBClassifier from xgboost to build model\nxg_boost = XGBClassifier(eval_metric=\"logloss\", random_state=1)\nxg_boost.fit(X_train, Y_train)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Create confusion matrix based on test data set\nmake_confusion_matrix(xg_boost)\n\n# Check performance of model on both training and test data sets\nperf_xg_boost = get_metrics_score(xg_boost)\nperf_xg_boost","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"##### Observations\n- The model is slightly overfitting because its performance is better on the training data set than on the test data set.\n- Hyperparameter tuning could be used to see if further improvement is possible.","metadata":{}},{"cell_type":"markdown","source":"### XGBoost Classifier with Hyperparameter Tuning","metadata":{}},{"cell_type":"code","source":"# Choose type of classifier\ntnd_xg_boost = XGBClassifier(eval_metric=\"logloss\", random_state=1)\n\n# Form grid of parameters to search in\ngrid_para = {\n    \"subsample\": [0.8, 1.0],\n    \"scale_pos_weight\": [1, 2],\n    \"gamma\": [3, 5],\n    \"colsample_bytree\": [0.8, 1.0],\n    \"colsample_bylevel\": [0.8, 1.0],\n    \"n_estimators\": [50, 100],\n    \"learning_rate\": [0.1, 0.2],\n}\n\n# Set type of score used to evaluate performance throughout search\nscorer = make_scorer(f1_score)\n\n# Run GridSearch\ngrid_obj = GridSearchCV(tnd_xg_boost, grid_para, scoring=scorer, cv=5)\ngrid_obj = grid_obj.fit(X_train, Y_train)\n\n# Set classifer to best combination of parameters\ntnd_xg_boost = grid_obj.best_estimator_\n\n# Fit best decision tree to training data\ntnd_xg_boost.fit(X_train, Y_train)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Create confusion matrix based on test data set\nmake_confusion_matrix(tnd_xg_boost)\n\n# Check performance of model on both training and test data sets\nperf_tnd_xg_boost = get_metrics_score(tnd_xg_boost)\nperf_tnd_xg_boost","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"##### Observations\n- The tuned XGBoost model provides similar performances on both the training and test data sets.\n- The model's performance on the test set was improved slightly via tuning, increasing the F1-score from 0.81 to 0.82.","metadata":{}},{"cell_type":"code","source":"# Check importances of various features of tuned XGBoost classifier\nimportances = tnd_xg_boost.feature_importances_\nindices = np.argsort(importances)\n\nplt.figure(figsize=(6, 0.5 * len(col_names)))\nplt.barh(range(len(indices)), importances[indices], color=\"violet\", align=\"center\")\nplt.yticks(range(len(indices)), [col_names[i] for i in indices])\nplt.xlabel(\"Relative Importance\", fontsize=15)\nplt.ylabel(\"Feature\", fontsize=15)\nplt.show()","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"##### Observations\n- The top four independent features of importance in the tuned XGBoost model are `education_of_employee`, `unit_of_wage_Year`, `has_job_experience`, and `continent_Europe`. ","metadata":{}},{"cell_type":"markdown","source":"# Stacking Classifier","metadata":{}},{"cell_type":"code","source":"# Use function XGBClassifier from sklearn to build model\nstacking = StackingClassifier(\n    estimators=[\n        (\"Decision Tree\", tnd_dcsn_tree),\n        (\"Random Forest\", tnd_rndm_frst),\n        (\"AdaBoost\", tnd_ada_boost),\n        (\"Gradient Boosting\", tnd_grdnt_boost),\n    ],\n    final_estimator=tnd_xg_boost,\n)\nstacking.fit(X_train, Y_train)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Create confusion matrix based on test data set\nmake_confusion_matrix(stacking)\n\n# Check performance of model on both training and test data sets\nperf_stacking = get_metrics_score(stacking)\nperf_stacking","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"##### Observations\n- The stacking model has a similar performance to the tuned XGBoost in terms of all metrics. Specifically, the F1-score is 0.82 for both the training and test data sets.","metadata":{}},{"cell_type":"markdown","source":"# Comparison of Model Performances","metadata":{"id":"obvious-maine"}},{"cell_type":"code","source":"# Create a data frame with summary of model performance on training data set\nperf_train = pd.concat(\n    [\n        perf_dcsn_tree.loc[\"Training\"].T,\n        perf_tnd_dcsn_tree.loc[\"Training\"].T,\n        perf_bagging.loc[\"Training\"].T,\n        perf_tnd_bagging.loc[\"Training\"].T,\n        perf_rndm_frst.loc[\"Training\"].T,\n        perf_tnd_rndm_frst.loc[\"Training\"].T,\n        perf_ada_boost.loc[\"Training\"].T,\n        perf_tnd_ada_boost.loc[\"Training\"].T,\n        perf_grdnt_boost.loc[\"Training\"].T,\n        perf_tnd_grdnt_boost.loc[\"Training\"].T,\n        perf_xg_boost.loc[\"Training\"].T,\n        perf_tnd_xg_boost.loc[\"Training\"].T,\n        perf_stacking.loc[\"Training\"].T,\n    ],\n    axis=1,\n)\n\nperf_train.columns = [\n    \"Decision Tree\",\n    \"Tuned Decision Tree\",\n    \"Bagging\",\n    \"Tuned Bagging\",\n    \"Random Forest\",\n    \"Tuned Random Forest\",\n    \"AdaBoost\",\n    \"Tuned AdaBoost\",\n    \"Gradient Boosting\",\n    \"Tuned Gradient Boosting\",\n    \"XGBoost\",\n    \"Tuned XGBoost\",\n    \"Stacking\",\n]\n\nprint(\"Model Performance Comparison for Training Data Set:\")\nperf_train","metadata":{"id":"everyday-kinase"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"##### Observations\n- Among the examined classifiers, *Decision Tree*, *Bagging*, *Tuned Bagging*, and *Random Forest* are overfitting the training data set.\n- The remaining models perform almost similarly in terms of F1-score, except *XGBoost* that outperforms others.","metadata":{}},{"cell_type":"code","source":"# Create a data frame with summary of model performance on training data set\nperf_test = pd.concat(\n    [\n        perf_dcsn_tree.loc[\"Test\"].T,\n        perf_tnd_dcsn_tree.loc[\"Test\"].T,\n        perf_bagging.loc[\"Test\"].T,\n        perf_tnd_bagging.loc[\"Test\"].T,\n        perf_rndm_frst.loc[\"Test\"].T,\n        perf_tnd_rndm_frst.loc[\"Test\"].T,\n        perf_ada_boost.loc[\"Test\"].T,\n        perf_tnd_ada_boost.loc[\"Test\"].T,\n        perf_grdnt_boost.loc[\"Test\"].T,\n        perf_tnd_grdnt_boost.loc[\"Test\"].T,\n        perf_xg_boost.loc[\"Test\"].T,\n        perf_tnd_xg_boost.loc[\"Test\"].T,\n        perf_stacking.loc[\"Test\"].T,\n    ],\n    axis=1,\n)\n\nperf_test.columns = [\n    \"Decision Tree\",\n    \"Tuned Decision Tree\",\n    \"Bagging\",\n    \"Tuned Bagging\",\n    \"Random Forest\",\n    \"Tuned Random Forest\",\n    \"AdaBoost\",\n    \"Tuned AdaBoost\",\n    \"Gradient Boosting\",\n    \"Tuned Gradient Boosting\",\n    \"XGBoost\",\n    \"Tuned XGBoost\",\n    \"Stacking\",\n]\n\nprint(\"Model Performance Comparison for Test Data Set:\")\nperf_test","metadata":{"id":"everyday-kinase"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"##### Observations\n- *Tuned Gradient Boosting* model slightly outperforms all other models in terms of F1-score. \n- However, *Tuned Decision Tree*, *Tuned Bagging*, *Tuned Random Forest*, *AdaBoost*, *Tuned AdaBoost*, *Gradient Boosting*, *Tuned Gradient Boosting*, *XGBoost*, *Tuned XGBoost*, and *Stacking* all provide close F1-scores (0.81-0.82).\n\n##### Selection of Final Model\n- Considering the model performance, its interpretability, and its simplicity altogether, the **tuned decision tree** is selected as the final model.","metadata":{}},{"cell_type":"markdown","source":"# Final Model","metadata":{"id":"amino-prediction"}},{"cell_type":"markdown","source":"#### Visualization","metadata":{}},{"cell_type":"code","source":"# Plot tuned tree\nplt.figure(figsize=(35, 10))\n\nplot_tree(\n    decision_tree=tnd_dcsn_tree,\n    feature_names=col_names,\n    filled=True,\n    fontsize=10,\n    node_ids=True,\n    class_names=True,\n)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Important Features","metadata":{}},{"cell_type":"code","source":"# Check importances of various features of tuned tree\nimportances = tnd_dcsn_tree.feature_importances_\nindices = np.argsort(importances)\n\nplt.figure(figsize=(6, 0.5 * len(col_names)))\nplt.barh(range(len(indices)), importances[indices], color=\"violet\", align=\"center\")\nplt.yticks(range(len(indices)), [col_names[i] for i in indices])\nplt.xlabel(\"Relative Importance\", fontsize=15)\nplt.ylabel(\"Feature\", fontsize=15)\nplt.show()","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Insights and Recommendations\n### Insights\n\n- According to the EDA:\n    - The majority (66%) of work via applications are from Asia.\n    - A large portion (78%) of the applicants have a bachelor's or a master's degree and only less than 9% have a doctrate degree.\n    - Most (58%) of the applicants have job experience.\n    - The vast majority of offerred jobs (88%) do not require training.\n    - The majority (>81%) of the offered jobs are for Northeast, South, and West regions of the US.\n    - The majority (89%) of the offered positions are full-time.\n    - Merely about 10% of the positions have a wage unit other than Year.\n    - About 2/3 of the work visa applications are certified.\n    - The European and South American applicants have the highest and the lowest chances of visa certification, respectively.\n    - The higher the applicant's education level is, the more their chances of visa certification are.\n    - Having job experience increases the chances of visa certification.\n    - Job training requirement has a negligible effect on visa certification likelihood.\n    - The visa applications for the employment in the Midwest region are more likely to be certified than the applications for the employment in other regions.\n    - Being a full- or part-time position does not observably affect the visa certification likelihood.\n    - The offered positions with the wage units of Year and Hour have the highest and the lowest chances of visa certification, respectively.\n    - The employer's number of employees has an insignificant impact on the chances of visa certification for its potential foreign employees.\n    - The majority of employers applying for work visas are less than 40 years old.\n    - The majority of the applications are for the jobs with an equivalent hourly wage of less than 100 (probably in dollars).\n    - The positions with certified visa applications are on average of lower equivalent hourly wages than the positions with denied visa applications.\n    - The age of an employer has negligible effect on the likelihood of visa certification.\n\n- According to the fitted classifiers:\n    - Almost all the classifiers perform similarly, but the *Tuned Gradient Boosting* model slightly outperforms other models in terms of F1-score - it provided the maximum F1-score of 0.822 on the test data. \n    - Overall, the features `education_of_employee`, `has_job_experience`, and `unit_of_wage_Year` are among the top four important variables affecting the visa certification likelihood. Other variables of importance are `continent_Europe` and `hourly_wage`.\n    - According to the final selected model, i.e., *Tuned Decision Tree*:\n        - The top four variables of importance when predicting a visa certification are `education_of_employee`, `has_job_experience`, `unit_of_wage_Year`, and `continent_Europe`.\n        - The applicants meeting the following criteria have high chances of visa *certification*:\n            - Having a master's or a doctorate degree (`education_of_employee` > 2.5); having job experience (`has_job_experience` > 0.5); and applying for a position with a prevailing wage unit of year (`unit_of_wage_Year` > 0.5)\n            - Having a university degree (`education_of_employee` > 1.5); having no job experience (`has_job_experience` <= 0.5); applying for a position with a prevailing wage unit of year (`unit_of_wage_Year` > 0.5); and being from Europe (`continent_Europe` > 0.5)\n            - Having a bachelor's degree (1.5 < `education_of_employee` <= 2.5); having job experience (`has_job_experience` > 0.5); and applying for a position with a prevailing wage unit of year (`unit_of_wage_Year` > 0.5)\n        - The applicants meeting the following criteria have high chances of visa *denial*:\n            - Having a bachelor's or a master's degree (1.5 < `education_of_employee` <= 3.5); having no job experience (`has_job_experience` <= 0.5); and applying for a position with a prevailing wage unit other than year (`unit_of_wage_Year` <= 0.5)\n            - Having no university degree (`education_of_employee` <= 1.5); being from Asia (`continent_Asia` > 0.5); and being employed in the West region (`region_of_employment_West` > 0.5)\n            - Having no university degree (`education_of_employee` <= 1.5); being from Asia (`continent_Asia` > 0.5); and being employed in the Northeast region (`region_of_employment_Northeast` > 0.5)\n            \n\n### Recommendations\n- Considering its relative simplicity and interpretability, the *Tuned Decision Tree* model is recommended to OFLC as the final classifier. If an ensemble model is preferred for reducing the bias, the *Tuned Gradient Boosting* model is recommended.\n- Given the above insights, OFLC shall particularly consider the applicants' level of education, their job experience, and their prevailing wage unit in its visa certification probability estimations. The applicants who have a higher education, have job experience, and their US employment's wage unit is year are more likely to be eventually certified for a work visa. Being from Europe also increases the chances of visa certification in certain cases.\n- In order to avoid workforce shortage in the US, especially in high-demand industries that depend on foreign employees, it is recommended that OFLC prioritizes the processing of the visa applications that have higher chances of certification based on the developed classification models.\n- To minimize the waste of OFLC's resources, it could quickly deny the applications that have very high chances of denial based on the prediction models - such applications could be reprocessed by a different section if appealed by the applicants/employers.\n- It is recommended that some other potentially important variables are also considered in the classification model development - examples are the industry of employment (e.g., medical, engineering, finance, agriculture, etc.), the applicant's amount of experience (e.g., in years), the agreement of the applicant's qualifications with the job, and the employer's socioeconomic benefits to the US.\n- More sophisticated ML-based classification models are also recommended to be tried for this purpose.","metadata":{"id":"nasty-retailer"}},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]}]}